{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title foo\n",
    "#!pip install transformers==4.1.1 plotnine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import seaborn\n",
    "import matplotlib\n",
    "\n",
    "from ahviz import create_indices, create_dataframe, filter_mask\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to force CPU if you have a GPU but not enough memory to do what you want. it will be slow of course\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformer = \"distilbert-base-cased\"\n",
    "#transformer = \"bert-base-cased\"\n",
    "#transformer = \"gpt2\"\n",
    "#transformer = \"gpt2-medium\"\n",
    "transformer = \"gpt2-large\"\n",
    "#transformer = \"twmkn9/bert-base-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer)\n",
    "# gpt2 doesn't do padding, so invent a padding token\n",
    "# this one was suggested by the error you get when trying\n",
    "# to do masking below, but it shouldn't matter as the actual\n",
    "# tokens get ignored by the attention mask anyway\n",
    "if transformer in ['gpt2', 'gpt2-medium', 'gpt2-large']:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModel.from_pretrained(transformer, output_attentions=True, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation\n",
    "\n",
    "Read in the prepared data. Included in the repository is a copy of the penn treebank sample that is included in the `nltk` python package, converted into plain text and split into sentences. But you can replace this with any\n",
    "text file. Since the first thing we do is join all the text, it isn't even neccessary to split it into sentences.\n",
    "\n",
    "The script I used to create the file is `convert_corpus.py` in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = pd.read_csv(\"firsthalf.txt\", sep=\"\\t\", header=None, names=[\"line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_b = pd.read_csv(\"secondhalf.txt\", sep=\"\\t\", header=None, names=[\"line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test things out, only use the first 100 lines of the datasets, so everything will go faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = dataset_a.head(100)\n",
    "dataset_b = dataset_b.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window size and context\n",
    "\n",
    "We move a sliding window over the complete dataset so we can always have context around the part we are looking at. This sets up how many tokens the model looks at each step, and with what step size to move through the corpus\n",
    "\n",
    "- `window_size`:  \n",
    "    the number of tokens that are in context\n",
    "\n",
    "- `step`:  \n",
    "    how many tokens we move ahead in each step through the corpus\n",
    "\n",
    "- `future`:  \n",
    "    how many tokens the model can look ahead\n",
    "    \n",
    "The mask printed below shows the effect of changing these values. The ones are the tokens we calculate things for, and the zeros are the extra context that the tokens of interest can pay attention to. For models like *GPT2*, `future` should be $0$, as the model only looks back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "step = 12\n",
    "future = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = []\n",
    "for half in [dataset_a, dataset_b]:\n",
    "    tokenized_sents = tokenizer(half['line'].tolist(), add_special_tokens=False)['input_ids']\n",
    "    if not \"gpt\" in transformer:\n",
    "        separated = map(lambda s: s + [tokenizer.sep_token_id], tokenized_sents)\n",
    "    else:\n",
    "        separated = tokenized_sents\n",
    "    chained = list(itertools.chain.from_iterable(separated))\n",
    "    tokens = torch.tensor(chained)\n",
    "    pad_len = window_size - len(tokens) % window_size\n",
    "    padded = torch.cat((tokens, tokens.new_full((pad_len,), tokenizer.pad_token_id)))\n",
    "    input_tensors.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.cat((torch.zeros(window_size - (step + future)), torch.ones(step), torch.zeros(future))).expand((100,-1))[0]\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(input_tensor:torch.Tensor, size: int, step: int, batch_size :int = 2):\n",
    "    input_ids = input_tensor.unfold(0, size, step)\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(input_ids)\n",
    "    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance functions\n",
    "\n",
    "This defines a few distance functions and their name. To select a different one, change the distance variable to one of the keys in the map. The name is used below in the diffence plot title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_map = {\n",
    "    'weighted': lambda d, w: d * w,\n",
    "    'weighted absolute': lambda d, w: np.abs(d) * w,\n",
    "    'weighted square': lambda d, w: np.square(d) * w,\n",
    "}\n",
    "\n",
    "distance = \"weighted square\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = None\n",
    "\n",
    "for n, dataset in enumerate(input_tensors):\n",
    "    dl = get_batches(dataset, window_size, step, batch_size=3)\n",
    "\n",
    "    data = None\n",
    "    for batch, t in enumerate(dl):\n",
    "        input_dict = {k: v.to(device) for k, v in zip([\"input_ids\"], t)}\n",
    "\n",
    "        output = model(**input_dict)\n",
    "\n",
    "        att = np.array([a.cpu().detach().numpy() for a in output['attentions']])\n",
    "\n",
    "        # swap the 'head' and 'sample' axes so they're in a more natural order\n",
    "        att = np.swapaxes(att, 1, 2)\n",
    "        ix = create_indices(att, sample=batch*att.shape[2])\n",
    "        df = create_dataframe(att, ix)\n",
    "        filtered = df[(df['from_token']>(window_size-(step+future))) & (df['from_token']<=(window_size-future)) ].copy()\n",
    "        filtered['distance'] = (filtered['to_token'] - filtered['from_token'])\n",
    "        filtered['sign'] = filtered['distance'] > 0\n",
    "        filtered['weighted'] = func_map[distance](filtered['distance'], filtered['attention_fraction'])\n",
    "        g = filtered.groupby(['layer', 'head', 'sample'])\n",
    "        grouped = (g['weighted'].agg([np.mean, 'count'])).reset_index()\n",
    "\n",
    "        if data is None:\n",
    "            data = grouped\n",
    "        else:\n",
    "            data = pd.concat([data, grouped])\n",
    "\n",
    "    data['dataset'] = n\n",
    "    \n",
    "    if result is None:\n",
    "        result = data\n",
    "    else:\n",
    "        result = pd.concat([result, data])\n",
    "\n",
    "df = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the data ready to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the weighted distances and their mean per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby(['dataset', 'layer', 'head'])\n",
    "avg_dist = (g['mean'].mean()).reset_index().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = avg_dist.pivot(index=['layer', 'head'], columns='dataset', values=\"mean\").reset_index()\n",
    "pivoted['diff'] = pivoted[0] - pivoted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l, h = avg_dist['dataset'].max() + 1, avg_dist['layer'].max(), avg_dist['head'].max()\n",
    "print(d,l,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_dist = avg_dist.sort_values([\"dataset\", \"layer\", \"mean\"]) \n",
    "sorted_avg_dist['sorted_head'] = np.tile(np.tile(np.arange(h) + 1, l), d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge this sorted_head column into the original data too\n",
    "data_sh = df.merge(sorted_avg_dist[['dataset', 'layer', 'head', 'sorted_head']], on=[\"dataset\", \"layer\", \"head\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average distances and the difference between the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(1, 3, figsize=(16, 6), sharey=True)\n",
    "fig.suptitle(f'average {distance} distance per head for the two datasets, and the difference per head')\n",
    "\n",
    "seaborn.heatmap(\n",
    "        ax=axes[0],\n",
    "        data=avg_dist[avg_dist['dataset'] == 0].pivot('layer', 'head', \"mean\"),\n",
    "        cmap=seaborn.light_palette(\"seagreen\", as_cmap=True)\n",
    "    )\n",
    "axes[0].set_title(\"dataset A\")\n",
    "\n",
    "seaborn.heatmap(\n",
    "        ax=axes[1],\n",
    "        data=pivoted.pivot(['layer'], 'head', 'diff'),\n",
    "        cmap=seaborn.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    )\n",
    "axes[1].set_title(\"difference\")\n",
    "\n",
    "seaborn.heatmap(\n",
    "        ax=axes[2],\n",
    "        data=avg_dist[avg_dist['dataset'] == 1].pivot('layer', 'head', \"mean\"),\n",
    "        cmap=seaborn.light_palette(\"seagreen\", as_cmap=True)\n",
    "    )\n",
    "axes[2].set_title(\"dataset B\")\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distances again, but with the heads sorted by the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(20, 12), sharey=True)\n",
    "fig.suptitle('average {distance} per head for two datasets, with the heads sorted per layer')\n",
    "\n",
    "seaborn.heatmap(\n",
    "        ax=axes[0],\n",
    "        data=sorted_avg_dist[sorted_avg_dist['dataset'] == 0].pivot('layer', 'sorted_head', \"mean\"),\n",
    "        cmap=seaborn.light_palette(\"seagreen\", as_cmap=True)\n",
    "    )\n",
    "axes[0].set_title(\"dataset A\")\n",
    "\n",
    "seaborn.heatmap(\n",
    "        ax=axes[1],\n",
    "        data=sorted_avg_dist[sorted_avg_dist['dataset'] == 1].pivot('layer', 'sorted_head', \"mean\"),\n",
    "        cmap=seaborn.light_palette(\"seagreen\", as_cmap=True)\n",
    "    )\n",
    "axes[1].set_title(\"dataset B\")\n",
    "\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and plots showing the distribution of the distance values\n",
    "\n",
    "First with the heads in model-order, and then again with the heads sorted by average distance.\n",
    "\n",
    "These are **really** slow, so you may want to skip running them (each cell took half an hour on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def make_violin(y, **kwargs):\n",
    "    v = seaborn.violinplot(y=y,x=\"layer\", hue=\"dataset\", split=True, **kwargs)\n",
    "    data = kwargs['data']\n",
    "    for dataset in range(2):\n",
    "        mean = np.round(np.mean(data[data['dataset'] == dataset][y]), 2)\n",
    "        v.text(-0.4 + (dataset * 0.5), 10, str(mean), fontdict=dict(color=\"red\", fontsize=30))\n",
    "    return v\n",
    "g = seaborn.FacetGrid(data_sh, col=\"head\",  row=\"layer\", col_order=(np.arange(h) + 1), row_order=np.flip(np.arange(l) + 1))\n",
    "g.map_dataframe(make_violin, \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = seaborn.FacetGrid(data_sh, col=\"sorted_head\",  row=\"layer\", col_order=(np.arange(h) + 1), row_order=np.flip(np.arange(l) + 1))\n",
    "g.map_dataframe(make_violin, \"value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
