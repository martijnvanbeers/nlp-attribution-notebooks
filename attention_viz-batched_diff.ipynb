{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# @title foo\n",
    "#!pip install transformers==4.1.1 plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import seaborn\n",
    "import matplotlib\n",
    "\n",
    "from ahviz import create_indices, create_dataframe, filter_mask\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this if you run into memory issues on the gpu\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = \"bert-base-cased\"\n",
    "#transformer = \"gpt2\"\n",
    "#transformer = \"gpt2-medium\"\n",
    "transformer = \"gpt2-large\"\n",
    "#transformer = \"twmkn9/bert-base-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer)\n",
    "\n",
    "# gpt2 doesn't do padding, so invent a padding token\n",
    "# this one was suggested by the error you get when trying\n",
    "# to do masking below, but it shouldn't matter as the actual\n",
    "# tokens get ignored by the attention mask anyway\n",
    "if transformer in ['gpt2', 'gpt2-medium', 'gpt2-large']:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModel.from_pretrained(transformer, output_attentions=True, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = pd.read_csv(\"firsthalf.txt\", sep=\"\\t\", header=None, names=[\"line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_b = pd.read_csv(\"secondhalf.txt\", sep=\"\\t\", header=None, names=[\"line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test things out, only use the first 100 lines of the datasets, so everything will go faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = dataset_a.head(100)\n",
    "dataset_b = dataset_b.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a synthetic dataset\n",
    "#dataset_b = pd.DataFrame([\"one two three four five six seven eight nine ten\"] * 100, columns=[\"line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "step = 25\n",
    "future = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = []\n",
    "for half in [dataset_a, dataset_b]:\n",
    "    tokenized_sents = tokenizer(half['line'].tolist(), add_special_tokens=False)['input_ids']\n",
    "    if not \"gpt\" in transformer:\n",
    "        separated = map(lambda s: s + [tokenizer.sep_token_id], tokenized_sents)\n",
    "    else:\n",
    "        separated = tokenized_sents\n",
    "    chained = list(itertools.chain.from_iterable(separated))\n",
    "    tokens = torch.tensor(chained)\n",
    "    pad_len = window_size - len(tokens) % window_size\n",
    "    padded = torch.cat((tokens, tokens.new_full((pad_len,), tokenizer.pad_token_id)))\n",
    "    input_tensors.append(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.cat((torch.zeros(window_size - (step + future)), torch.ones(step), torch.zeros(future))).expand((100,-1))[0]\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(input_tensor:torch.Tensor, size: int, step: int, batch_size :int = 2):\n",
    "    input_ids = input_tensor.unfold(0, size, step)\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(input_ids)\n",
    "    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = None\n",
    "\n",
    "for n, dataset in enumerate(input_tensors):\n",
    "    dl = get_batches(dataset, window_size, step, batch_size=3)\n",
    "\n",
    "    data = None\n",
    "    for batch, t in enumerate(dl):\n",
    "        input_dict = {k: v.to(device) for k, v in zip([\"input_ids\"], t)}\n",
    "\n",
    "        output = model(**input_dict)\n",
    "\n",
    "        att = np.array([a.cpu().detach().numpy() for a in output['attentions']])\n",
    "\n",
    "        # sort all the attention softmax vectors in descending order\n",
    "        sorted = np.take_along_axis(att, (-att).argsort(), axis=-1)\n",
    "\n",
    "        # add them up cumulatively\n",
    "        cum = sorted.cumsum(axis=-1)\n",
    "\n",
    "        # determine which ones are below 0.9\n",
    "        limit = np.where(cum < 0.9, True, False)\n",
    "\n",
    "        # count the ones below 0.9; k is that sum + 1\n",
    "        k = limit.sum(axis=-1) + 1\n",
    "\n",
    "        # swap the 'head' and 'sentence' axes so we can more easily apply the attention mask\n",
    "        ks = np.swapaxes(k, 1, 2)\n",
    "\n",
    "        if data is None:\n",
    "            data = ks\n",
    "        else:\n",
    "            data = np.concatenate([data, ks], axis=2)\n",
    "    ix = create_indices(data, names=['layer', 'head', 'sample', 'from_token'])\n",
    "    df = create_dataframe(data, ix)\n",
    "    df['dataset'] = n\n",
    "    if result is None:\n",
    "        result = df\n",
    "    else:\n",
    "        result = pd.concat([result, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered = result[(result['from_token']>(window_size-(step+future))) & (result['from_token']<=(window_size-future)) ].rename(columns={'attention_fraction': \"value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_k = filtered.groupby(['dataset', 'layer', 'head']).agg(avg_k = pd.NamedAgg('value', np.median)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = avg_k.pivot(index=['layer', 'head'], columns='dataset', values=\"avg_k\").reset_index()\n",
    "pivoted['diff'] = pivoted[0] - pivoted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(1, 3, figsize=(16, 6), sharey=True)\n",
    "fig.suptitle('average K per head for the two datasets, and the difference per head')\n",
    "\n",
    "seaborn.heatmap(ax=axes[0], data=avg_k[avg_k['dataset'] == 0].pivot('layer', 'head', \"avg_k\"), cmap=seaborn.light_palette(\"seagreen\", as_cmap=True))\n",
    "axes[0].set_title(\"dataset A\")\n",
    "\n",
    "seaborn.heatmap(ax=axes[1], data=pivoted.pivot(['layer'], 'head', 'diff'), cmap=seaborn.color_palette(\"coolwarm\", as_cmap=True))\n",
    "axes[1].set_title(\"difference\")\n",
    "\n",
    "seaborn.heatmap(ax=axes[2], data=avg_k[avg_k['dataset'] == 1].pivot('layer', 'head', \"avg_k\"), cmap=seaborn.light_palette(\"seagreen\", as_cmap=True))\n",
    "axes[2].set_title(\"dataset B\")\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_k.pivot(['dataset','layer'], 'head', \"avg_k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To replicate the plot in the hopfield network paper better, add a `sorted_head` column just so we can plot the attention heads per layer sorted from small to large k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, l, h = avg_k['dataset'].max() + 1, avg_k['layer'].max(), avg_k['head'].max()\n",
    "print(d,l,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_k = avg_k.sort_values([\"dataset\", \"layer\", \"avg_k\"]) \n",
    "sorted_avg_k['sorted_head'] = np.tile(np.tile(np.arange(h) + 1, l), d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(20, 12), sharey=True)\n",
    "fig.suptitle('average K per head for two datasets, with the heads sorted per layer')\n",
    "\n",
    "seaborn.heatmap(ax=axes[0], data=sorted_avg_k[sorted_avg_k['dataset'] == 0].pivot('layer', 'sorted_head', \"avg_k\"), cmap=seaborn.light_palette(\"seagreen\", as_cmap=True))\n",
    "axes[0].set_title(\"dataset A\")\n",
    "\n",
    "seaborn.heatmap(ax=axes[1], data=sorted_avg_k[sorted_avg_k['dataset'] == 1].pivot('layer', 'sorted_head', \"avg_k\"), cmap=seaborn.light_palette(\"seagreen\", as_cmap=True))\n",
    "axes[1].set_title(\"dataset B\")\n",
    "\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge this sorted_head column into the original data too\n",
    "data_sh = filtered.merge(sorted_avg_k[['dataset', 'layer', 'head', 'sorted_head']], on=[\"dataset\", \"layer\", \"head\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Plots\n",
    "\n",
    "## first in the natural order of the layers/heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def make_violin(y, **kwargs):\n",
    "    v = seaborn.violinplot(y=y,x=\"layer\", hue=\"dataset\", split=True, **kwargs)\n",
    "    data = kwargs['data']\n",
    "    for dataset in range(2):\n",
    "        median_k = np.median(data[data['dataset'] == dataset][y])\n",
    "        mean = np.mean(data[data['dataset'] == dataset][y])\n",
    "        v.text(-0.4 + (dataset * 0.5), 10, str(median_k), fontdict=dict(color=\"red\", fontsize=30))\n",
    "    return v\n",
    "g = seaborn.FacetGrid(data_sh, col=\"head\",  row=\"layer\", col_order=(np.arange(h) + 1), row_order=np.flip(np.arange(l) + 1))\n",
    "g.map_dataframe(make_violin, \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the heads per layer sorted by the median k, like in the hopfield networks paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = seaborn.FacetGrid(data_sh, col=\"sorted_head\",  row=\"layer\", col_order=(np.arange(h) + 1), row_order=np.flip(np.arange(l) + 1))\n",
    "g.map_dataframe(make_violin, \"value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
