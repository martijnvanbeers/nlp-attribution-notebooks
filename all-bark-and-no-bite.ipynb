{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437f66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from scipy import corrcoef\n",
    "from scipy.spatial.distance import cosine, euclidean, pdist, squareform, is_valid_dm, cdist\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial import distance_matrix\n",
    "import torch\n",
    "\n",
    "#Visualization packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098ba6e",
   "metadata": {},
   "source": [
    "# Implementing [All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality](https://arxiv.org/abs/2109.04404)\n",
    "\n",
    "Numpy implementation of the formulas in the paper. The quoted text is copied out of the paper for context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a603656",
   "metadata": {},
   "source": [
    "> [ethayarajh (2019)](https://aclanthology.org/D19-1006/) defines the anisotropy in layer $\\ell$ of model $f$ as the expected cosine similarity of any pair of words in a corpus.\n",
    "This can be approximated as $\\textit{$\\hat{A}$} (f_{\\ell})$ from a sample $S$ of $n$ random token pairs from a corpus $\\mathcal{O}$.\n",
    "\n",
    "> $$S = \\{\\{x_1,y_1\\},...,\\{x_n,y_n\\}\\} \\sim \\mathcal{O}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1aaf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300000 # number of samples\n",
    "d = 100 # vector dimension\n",
    "S = np.random.rand(n,2, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2cf45",
   "metadata": {},
   "source": [
    "> $$\\hat{A}(f_{\\ell}) = \\frac{1}{n} \\sum_{\\{x_\\alpha,y_\\alpha\\} \\in S} \\cos( f_{\\ell}(x_\\alpha), f_{\\ell}(y_\\alpha) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12497d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "anisotropy = lambda S: (1/n) * np.sum([cos_uv(fx, fy) for fx, fy in S])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62440345",
   "metadata": {},
   "source": [
    "> $$ \\cos(u, v) = \\frac{u \\cdot v} {{\\lVert u \\rVert}{\\lVert v \\rVert}} = \\sum_{i=1}^d \\frac{ u_i v_i}{{\\lVert u \\rVert}{\\lVert v \\rVert}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80204890",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_uv = lambda u, v: u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72f582",
   "metadata": {},
   "source": [
    "> $$ CC_i(u, v) = \\frac{ u_i v_i}{{\\lVert u \\rVert}{\\lVert v \\rVert}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bd4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_uv = lambda u, v: u * v / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37a12c",
   "metadata": {},
   "source": [
    "> $$ {CC}(f^i_{\\ell}) = \\frac{1}{n} \\ \\cdot\\!\\!\\!\\!\\!\\!\\sum_{\\{x_\\alpha,y_\\alpha\\} \\in S} CC_i( f_{\\ell}(x_\\alpha), f_{\\ell}(y_\\alpha) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ac62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_fl = lambda S: (1/n) * np.sum(np.squeeze([cci_uv(fx, fy) for fx, fy in S]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26c2f",
   "metadata": {},
   "source": [
    "> Note that $\\sum^d_i{CC}(f^i_{\\ell}) = \\hat{A}(f_{\\ell})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da338848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(anisotropy(S), cc_fl(S).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca45dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.9 s, sys: 23.6 ms, total: 2.92 s\n",
      "Wall time: 2.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7506107466202372"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time anisotropy(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a93030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 129 ms, total: 3.63 s\n",
      "Wall time: 3.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00749749, 0.00750954, 0.00748971, 0.00751329, 0.00748488,\n",
       "       0.00750173, 0.00751275, 0.00750584, 0.00750578, 0.00749085,\n",
       "       0.00750159, 0.0074942 , 0.00750036, 0.00750421, 0.00749298,\n",
       "       0.00750623, 0.0075318 , 0.00751165, 0.00749315, 0.00751508,\n",
       "       0.00750017, 0.00751986, 0.00748115, 0.00750584, 0.00749662,\n",
       "       0.00750519, 0.00751023, 0.00750669, 0.00750762, 0.00750954,\n",
       "       0.00751803, 0.00749991, 0.00747264, 0.00750444, 0.00753296,\n",
       "       0.00748529, 0.00750528, 0.00751205, 0.00750904, 0.00750049,\n",
       "       0.00750696, 0.00748823, 0.00753414, 0.00750516, 0.00751942,\n",
       "       0.0075053 , 0.00751044, 0.00751752, 0.00750244, 0.00750581,\n",
       "       0.00751689, 0.00750857, 0.00750777, 0.00750515, 0.0075133 ,\n",
       "       0.00749719, 0.00750302, 0.00748344, 0.00749711, 0.00749886,\n",
       "       0.0074961 , 0.00750853, 0.00751339, 0.00749931, 0.00750472,\n",
       "       0.0075191 , 0.00751134, 0.00751343, 0.00752263, 0.00751289,\n",
       "       0.0074892 , 0.0074945 , 0.0075044 , 0.00750736, 0.00750627,\n",
       "       0.00751564, 0.00750305, 0.00749575, 0.00750283, 0.00750128,\n",
       "       0.00750559, 0.00750899, 0.00749273, 0.00751314, 0.00750994,\n",
       "       0.00751686, 0.0075242 , 0.00750239, 0.00750386, 0.00752587,\n",
       "       0.00750898, 0.00749819, 0.00751103, 0.00751949, 0.00750639,\n",
       "       0.00751319, 0.00750487, 0.00751019, 0.00751532, 0.00752496])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cc_fl(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68fa14",
   "metadata": {},
   "source": [
    "## vectorized version for speed\n",
    "\n",
    "Instead of looping over the samples, do the calculations with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0ce0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda S: np.linalg.norm(S, axis=2, keepdims=True).prod(axis=1)\n",
    "ccfl = lambda S: (1/S.shape[0]) * ((np.prod(S, axis=1) / norm(S))).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4691341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 44.2 ms, total: 279 ms\n",
      "Wall time: 278 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00747264, 0.00748115, 0.00748344, 0.00748488, 0.00748529,\n",
       "       0.00748823, 0.0074892 , 0.00748971, 0.00749085, 0.00749273,\n",
       "       0.00749298, 0.00749315, 0.0074942 , 0.0074945 , 0.00749575,\n",
       "       0.0074961 , 0.00749662, 0.00749711, 0.00749719, 0.00749749,\n",
       "       0.00749819, 0.00749886, 0.00749931, 0.00749991, 0.00750017,\n",
       "       0.00750036, 0.00750049, 0.00750128, 0.00750159, 0.00750173,\n",
       "       0.00750239, 0.00750244, 0.00750283, 0.00750302, 0.00750305,\n",
       "       0.00750386, 0.00750421, 0.0075044 , 0.00750444, 0.00750472,\n",
       "       0.00750487, 0.00750515, 0.00750516, 0.00750519, 0.00750528,\n",
       "       0.0075053 , 0.00750559, 0.00750578, 0.00750581, 0.00750584,\n",
       "       0.00750584, 0.00750623, 0.00750627, 0.00750639, 0.00750669,\n",
       "       0.00750696, 0.00750736, 0.00750762, 0.00750777, 0.00750853,\n",
       "       0.00750857, 0.00750898, 0.00750899, 0.00750904, 0.00750954,\n",
       "       0.00750954, 0.00750994, 0.00751019, 0.00751023, 0.00751044,\n",
       "       0.00751103, 0.00751134, 0.00751165, 0.00751205, 0.00751275,\n",
       "       0.00751289, 0.00751314, 0.00751319, 0.00751329, 0.0075133 ,\n",
       "       0.00751339, 0.00751343, 0.00751508, 0.00751532, 0.00751564,\n",
       "       0.00751686, 0.00751689, 0.00751752, 0.00751803, 0.0075191 ,\n",
       "       0.00751942, 0.00751949, 0.00751986, 0.00752263, 0.0075242 ,\n",
       "       0.00752496, 0.00752587, 0.0075318 , 0.00753296, 0.00753414])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.sort(ccfl(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9367f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(np.isclose(anisotropy(S), ccfl(S).sum()))\n",
    "print(np.isclose(cc_fl(S),ccfl(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b14da",
   "metadata": {},
   "source": [
    "## Testing it on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bb6ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\" #@param\n",
    "#MODEL_NAME = \"gpt2\" # doesn't quite work?\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME,\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d27bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_output_for_layers(model, inputs, states, word_groups, layers):\n",
    "    # Stack all words in the sentence\n",
    "    if MODEL_NAME in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]:\n",
    "        emb_layer = model.wte\n",
    "    else:\n",
    "        emb_layer = model.embeddings.word_embeddings\n",
    "        \n",
    "    sent_tokens_output = torch.stack([\n",
    "        # Sum the requested layers\n",
    "        torch.stack([\n",
    "                states[i].detach()[:,token_ids_word].mean(axis=1)\n",
    "                    if i > 0 else\n",
    "                emb_layer(inputs)[:,token_ids_word].mean(axis=1)\n",
    "                        for i in layers\n",
    "            ]).sum(axis=0).squeeze()\n",
    "                for token_ids_word in word_groups\n",
    "        ])\n",
    "#    print(\"OUTPUT SHAPE\", sent_tokens_output.shape)\n",
    "    return sent_tokens_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb441560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3913.000000\n",
       "mean       20.652952\n",
       "std         9.943947\n",
       "min         1.000000\n",
       "25%        13.000000\n",
       "50%        20.000000\n",
       "75%        27.000000\n",
       "max        80.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = pd.read_csv(\"lines.csv\")\n",
    "sentences['length'] = sentences.line.str.split().apply(len)\n",
    "display(sentences[sentences['length'] <100].length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f6d046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (np.arange(12) + 1).reshape(-1,1)\n",
    "from collections import defaultdict\n",
    "vecs = []\n",
    "layer_result = defaultdict(list)\n",
    "sent_words = []\n",
    "\n",
    "for sent in sentences['line']:\n",
    "    encoded = tokenizer(sent, return_tensors=\"pt\")\n",
    "    inputs = encoded.input_ids\n",
    "    attention_mask =  encoded['attention_mask']\n",
    "    output = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "    states = output.hidden_states\n",
    "    token_len = attention_mask.sum().item()\n",
    "    decoded = tokenizer.convert_ids_to_tokens(inputs[0], skip_special_tokens=False)\n",
    "    if MODEL_NAME in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]:\n",
    "        word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[:token_len]\n",
    "        word_groups = np.split(np.arange(word_indices.shape[0]), np.unique(word_indices, return_index=True)[1])[1:]\n",
    "        sw = [\"\".join(list(map(lambda t: t[1:] if t[:1] == \"Ġ\" else t, np.array(decoded)[g]))) for g in word_groups]\n",
    "        sent_words.append(sw)\n",
    "    else:\n",
    "        word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[1:token_len - 1]\n",
    "        word_groups = np.split(np.arange(word_indices.shape[0]) + 1, np.unique(word_indices, return_index=True)[1])[1:]\n",
    "        sent_words.append([\"\".join(list(map(lambda t: t[2:] if t[:2] == \"##\" else t, np.array(decoded)[g]))) for g in word_groups])\n",
    "\n",
    "    for n, layer_group in enumerate(layers):\n",
    "        sent_vec = combine_output_for_layers(model, inputs, states, word_groups, layer_group)\n",
    "        layer_result[n].append(sent_vec)\n",
    "\n",
    "vecs = [np.concatenate(r) for r in layer_result.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66fd5c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>𝐴̂(𝑓ℓ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.137615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.743</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.187472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.186794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.217777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.254179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.719</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.243871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.593</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.237626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.535</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.244794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.487</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.229058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.511</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.237548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.676</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.296828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.266</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.259996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1      2      3    𝐴̂(𝑓ℓ)\n",
       "1   0.589  0.077  0.026  0.137615\n",
       "2   0.743  0.013  0.008  0.187472\n",
       "3   0.716  0.010  0.008  0.186794\n",
       "4   0.735  0.009  0.008  0.217777\n",
       "5   0.754  0.014  0.009  0.254179\n",
       "6   0.719  0.031  0.020  0.243871\n",
       "7   0.593  0.113  0.025  0.237626\n",
       "8   0.535  0.184  0.027  0.244794\n",
       "9   0.487  0.187  0.027  0.229058\n",
       "10  0.511  0.114  0.022  0.237548\n",
       "11  0.676  0.025  0.017  0.296828\n",
       "12  0.266  0.021  0.012  0.259996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count = vecs[0].shape[0]\n",
    "sample_size = word_count // 5\n",
    "sample_size -= sample_size % 2\n",
    "sample = np.random.choice(word_count, sample_size, replace=False)\n",
    "result = []\n",
    "for layer_data in vecs:\n",
    "    data = layer_data[sample].reshape(sample_size // 2, 2, -1)\n",
    "#    print(\"shape\", data.shape)\n",
    "#    print(\"sum\", data[0][0].shape, data[0][0].sum())\n",
    "    cc = ccfl(data)\n",
    "#    print(cc.shape)\n",
    "    ani = cc.sum()\n",
    "#    print(\"ANI\", ani)\n",
    "    rel_cc = cc / ani\n",
    "    result.append(np.sort(rel_cc).round(3)[-3:][::-1].tolist() + [ani])\n",
    "    \n",
    "display(pd.DataFrame(result, columns = [\"1\", \"2\", \"3\", \"𝐴̂(𝑓ℓ)\"], index=layers.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa016b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
