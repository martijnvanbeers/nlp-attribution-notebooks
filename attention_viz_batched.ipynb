{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title foo\n",
    "!pip install transformers==4.1.1 plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "plotnine.options.figure_size = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotnine.options.figure_size = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = \"bert-base-cased\"\n",
    "#transformer = \"gpt2\"\n",
    "#transformer = \"gpt2-medium\"\n",
    "#transformer = \"gpt2-large\"\n",
    "#transformer = \"twmkn9/bert-base-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer)\n",
    "\n",
    "# gpt2 doesn't do padding, so invent a padding token\n",
    "# this one was suggested by the error you get when trying\n",
    "# to do masking below, but it shouldn't matter as the actual\n",
    "# tokens get ignored by the attention mask anyway\n",
    "if transformer in ['gpt2', 'gpt2-medium', 'gpt2-large']:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModel.from_pretrained(transformer, output_attentions=True, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the prepared data.\n",
    "# based on the penn treebank sample in nltk; prepared with the convert_corpus.py script\n",
    "sentences = pd.read_csv(\"lines.csv\")\n",
    "sentences['length'] = sentences.line.str.split().apply(len)\n",
    "display(sentences[sentences['length'] <100].length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[sentences['length'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(sentences.values[1854,0])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(df:pd.DataFrame, tokenizer, lengths: tuple = None, batch_size :int = 2):\n",
    "    \n",
    "    if lengths is not None:\n",
    "        subset = sentences[(sentences['length'] > lengths[0]) & (sentences['length'] < lengths[1])]\n",
    "    else:\n",
    "        subset = sentences\n",
    "    input_dict = tokenizer(subset['line'].values.tolist(), padding=True, return_tensors=\"pt\")\n",
    "    input_ids, token_type_ids, attention_mask = input_dict.values()\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(input_ids, token_type_ids, attention_mask)\n",
    "    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_batches(sentences, tokenizer, lengths=(25, 35), batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "for batch, t in enumerate(dl):\n",
    "    input_dict = {k: v.to(device) for k, v in zip([\"input_ids\", \"token_type_ids\", \"attention_mask\"], t)}\n",
    "    \n",
    "    output = model(**input_dict)\n",
    "\n",
    "    att = np.array([a.cpu().detach().numpy() for a in output['attentions']])\n",
    "    #print(att.shape)\n",
    "\n",
    "    # sort all the attention softmax vectors in descending order\n",
    "    sorted = np.take_along_axis(att, (-att).argsort(), axis=-1)\n",
    "\n",
    "    # add them up cumulatively\n",
    "    cum = sorted.cumsum(axis=-1)\n",
    "\n",
    "    # determine which ones are below 0.9\n",
    "    limit = np.where(cum < 0.9, True, False)\n",
    "\n",
    "    # count the ones below 0.9; k is that sum + 1\n",
    "    k = limit.sum(axis=-1) + 1\n",
    "\n",
    "    # swap the 'head' and 'sentence' axes so we can more easily apply the attention mask\n",
    "    ks = np.swapaxes(k, 1, 2)\n",
    "\n",
    "    # use the attention mask to flag the padding tokens\n",
    "    att_mask = input_dict['attention_mask'].cpu().detach()\n",
    "    mt = np.ma.MaskedArray(ks, mask = (att_mask == False).expand(ks.shape))\n",
    "\n",
    "    # flatten out the sentences so we're left with just a list of tokens\n",
    "    mr = mt.reshape(ks.shape[:2] + tuple([np.prod(ks.shape[2:])]))\n",
    "\n",
    "    # find the indices of the token list we're interested in\n",
    "    unmasked = np.flatnonzero(att_mask)\n",
    "\n",
    "    # get the dimensions of the data we want\n",
    "    # layer × head × #tokens\n",
    "    l, h, v = mr[:, :, unmasked].shape\n",
    "\n",
    "    # create a layer/head multiindex\n",
    "    ix = pd.MultiIndex.from_arrays(\n",
    "        [\n",
    "            np.repeat(np.arange(l) + 1,h),\n",
    "            np.tile(np.arange(h) + 1, l)\n",
    "        ], \n",
    "        names=['layer', 'head'])\n",
    "\n",
    "    # finally filter out the padding tokens, put the data in a dataframe,\n",
    "    # and transform it so we get one layer/head/token/k per row\n",
    "    batch_data = (\n",
    "            pd.DataFrame(mr[:,:,unmasked].reshape((l*h,len(unmasked))), index=ix)\n",
    "                .reset_index()\n",
    "                .melt(id_vars=['layer', 'head'])\n",
    "        )\n",
    "    batch_data['batch'] = batch\n",
    "    if data is None:\n",
    "        data = batch_data\n",
    "    else:\n",
    "        data = pd.concat([data, batch_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_k = data.groupby(['layer', 'head']).agg(avg_k = pd.NamedAgg('value', np.median)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the plot in the hopfield network paper better, add a `sorted_head` column just so we can plot the attention heads per layer sorted from small to large k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avg_k = avg_k.sort_values([\"layer\", \"avg_k\"]) \n",
    "sorted_avg_k['sorted_head'] = np.tile(np.arange(h) + 1, l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge this sorted_head column into the original data too\n",
    "data_sh = data.merge(sorted_avg_k[['layer', 'head', 'sorted_head']], on=[\"layer\", \"head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geom_violin can't deal with too much data, so instead of giving\n",
    "# it the raw data, count how often each value occurs and use\n",
    "# the count as weight\n",
    "gdata = data_sh.groupby(['layer', 'head', 'value']).agg({'variable': 'count'}).reset_index()\n",
    "\n",
    "# also for the sorted heads\n",
    "sgdata = data_sh.groupby(['layer', 'sorted_head', 'value']).agg({'variable': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of the avg_k value in the plot\n",
    "ypos = gdata['value'].max() * .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "## first in the natural order of the layers/heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it!\n",
    "(ggplot(gdata, aes(1, \"value\"))  + \n",
    "     geom_violin(mapping=aes(weight=\"variable\"), fill=\"pink\") + \n",
    "     geom_jitter(mapping=aes(colour=\"variable\"), alpha=0.5, size=1) +\n",
    "     geom_label(data=sorted_avg_k, mapping=aes(x=1, y=ypos, label=\"avg_k\")) +\n",
    "     scale_color_continuous(cmap_name=\"viridis_r\") +\n",
    "     facet_grid(\"layer ~ head\", labeller=\"label_both\") + \n",
    "     theme_dark() +\n",
    "     coord_flip() +\n",
    "     labs(\n",
    "             x = \"\",\n",
    "             y = \"k\",\n",
    "             title = \"Distribution and median k for each attention head\"\n",
    "         ) +\n",
    "     theme(\n",
    "             axis_text_y = element_blank(),\n",
    "             axis_ticks_major_y = element_blank()\n",
    "         )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the heads per layer sorted by the median k, like in the hopfield networks paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it!\n",
    "(ggplot(sgdata, aes(1, \"value\"))  + \n",
    "     geom_violin(mapping=aes(weight=\"variable\"), fill=\"pink\") + \n",
    "     geom_jitter(mapping=aes(colour=\"variable\"), alpha=0.5, size=1) +\n",
    "     geom_label(data=sorted_avg_k, mapping=aes(x=1, y=ypos, label=\"avg_k\")) +\n",
    "     scale_color_continuous(cmap_name=\"viridis_r\") +\n",
    "     facet_grid(\"layer ~ sorted_head\", labeller=\"label_both\") + \n",
    "     theme_dark() +\n",
    "     coord_flip() +\n",
    "     labs(\n",
    "             x = \"\",\n",
    "             y = \"attention\",\n",
    "             title = \"Distribution and median k for each attention head\"\n",
    "         ) +\n",
    "     theme(\n",
    "             axis_text_y = element_blank(),\n",
    "             axis_ticks_major_y = element_blank()\n",
    "         )\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
