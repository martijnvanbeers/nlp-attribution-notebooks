{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title foo\n",
    "!pip install transformers==4.1.1 captum==0.3.0 plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import DeepLiftShap, ShapleyValueSampling, LayerIntegratedGradients, IntegratedGradients, Occlusion\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "plotnine.options.figure_size = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = \"distilbert-base-cased\"\n",
    "transformer = \"roberta-base\"\n",
    "#transformer = \"twmkn9/bert-base-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer)\n",
    "model = AutoModelForMaskedLM.from_pretrained(transformer)\n",
    "#model.eval()\n",
    "#model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_sentence(sent, N):\n",
    "    input_seq = tokenizer.encode(sent)\n",
    "\n",
    "    mask_index = input_seq.index(tokenizer.mask_token_id)\n",
    "#    print(mask_index)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq])\n",
    "\n",
    "    input_result = model.forward(input_tensor, return_dict=True)\n",
    "\n",
    "    input_result.logits.shape\n",
    "\n",
    "    token_logits = input_result.logits\n",
    "    mask_token_logits = token_logits[0, mask_index, :]\n",
    "    mask_token_probs = torch.nn.functional.softmax(mask_token_logits, dim=0)\n",
    "\n",
    "    # get the top predictions for the non-occluded sentence\n",
    "    top_N = torch.topk(mask_token_probs, N, dim=0)\n",
    "#    print(top_N)\n",
    "    probs = top_N.values.tolist()\n",
    "    top_N_tokens = top_N.indices.tolist()\n",
    "#    print(probs, sum(probs), top_N_tokens)\n",
    "    return input_seq, top_N_tokens, mask_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dls_run_attribution_model(input_seq, ref_token_id, top_N_tokens):\n",
    "    def custom_forward(inputs, attention_mask=None):\n",
    "        result = model.forward(inputs, attention_mask=attention_mask, return_dict=True)\n",
    "        preds = result.logits\n",
    "        N_token = preds[:, 0, :]\n",
    "        return N_token\n",
    "\n",
    "    ablator = DeepLiftShap(custom_forward)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq] * len(top_N_tokens))\n",
    "    attention_mask = torch.ones_like(input_tensor)\n",
    "#    print(input_tensor.shape, input_tensor.dtype)\n",
    "    ref_tensor = torch.tensor([ref_token_id]).expand((len(top_N_tokens), len(input_seq)))\n",
    "#    print(ref_tensor.shape, input_tensor.dtype)\n",
    "\n",
    "    attributions = ablator.attribute(\n",
    "            inputs=input_tensor,\n",
    "            baselines=ref_tensor,\n",
    "            additional_forward_args=(attention_mask,),\n",
    "            target=top_N_tokens,\n",
    "    )\n",
    "\n",
    "    return attributions.T\n",
    "\n",
    "def svs_run_attribution_model(input_seq, ref_token_id, top_N_tokens):\n",
    "    def custom_forward(inputs, attention_mask=None):\n",
    "        result = model.forward(inputs, attention_mask=attention_mask, return_dict=True)\n",
    "        preds = result.logits\n",
    "        N_token = preds[:, 0, :]\n",
    "        return N_token\n",
    "\n",
    "    ablator = ShapleyValueSampling(custom_forward)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq] * len(top_N_tokens))\n",
    "    attention_mask = torch.ones_like(input_tensor)\n",
    "#    print(input_tensor.shape, input_tensor.dtype)\n",
    "    ref_tensor = torch.tensor([ref_token_id]).expand((len(top_N_tokens), len(input_seq)))\n",
    "#    print(ref_tensor.shape, input_tensor.dtype)\n",
    "\n",
    "    attributions = ablator.attribute(\n",
    "            inputs=input_tensor,\n",
    "            baselines=ref_tensor,\n",
    "            additional_forward_args=(attention_mask,),\n",
    "            target=top_N_tokens,\n",
    "    )\n",
    "\n",
    "    return attributions.T\n",
    "\n",
    "def occlusion_run_attribution_model(input_seq, ref_token_id, top_N_tokens):\n",
    "    def custom_forward(inputs, attention_mask=None):\n",
    "        #result = model.forward(inputs.long(), return_dict=True)\n",
    "        result = model.forward(inputs, return_dict=True)\n",
    "        preds = result.logits\n",
    "        N_token = preds[:, 0, :]\n",
    "        return N_token\n",
    "\n",
    "    ablator = Occlusion(custom_forward)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq] * len(top_N_tokens))\n",
    "\n",
    "    ref_tensor = torch.tensor([ref_token_id])\n",
    "\n",
    "\n",
    "    attributions = ablator.attribute(\n",
    "            inputs=input_tensor,\n",
    "            baselines=ref_token_id,\n",
    "            sliding_window_shapes=(1,),\n",
    "            target=top_N_tokens,\n",
    "    )\n",
    "#    print(\"ATTRIBUTIONS\", attributions.T)\n",
    "    return attributions.T\n",
    "\n",
    "def lig_run_attribution_model(input_seq, ref_token_id, top_N_tokens):\n",
    "    def custom_forward(inputs, attention_mask=None):\n",
    "        result = model.forward(inputs, attention_mask=attention_mask, return_dict=True)\n",
    "        preds = result.logits\n",
    "        N_token = preds[:, 0, :]\n",
    "        return N_token\n",
    "\n",
    "    def summarize_attributions(attributions):\n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        attributions = attributions / torch.norm(attributions)\n",
    "        return attributions\n",
    "    ablator = LayerIntegratedGradients(custom_forward, model.roberta.embeddings)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq] * len(top_N_tokens))\n",
    "#    attention_mask = torch.ones_like(input_tensor)\n",
    "\n",
    "    ref_tensor = torch.tensor([ref_token_id])\n",
    "#    print(top_N_tokens)\n",
    "\n",
    "\n",
    "    attributions = ablator.attribute(\n",
    "            inputs=input_tensor,\n",
    "            baselines=ref_token_id,\n",
    "#            additional_forward_args=(attention_mask,),\n",
    "            target=top_N_tokens,\n",
    "    )\n",
    "    attributions = summarize_attributions(attributions)\n",
    "    return attributions.T\n",
    "\n",
    "def ig_run_attribution_model(input_seq, ref_token_id, top_N_tokens):\n",
    "    def custom_forward(inputs, attention_mask=None):\n",
    "        result = model.forward(inputs, return_dict=True, attention_mask=attention_mask)\n",
    "        preds = result.logits\n",
    "        N_token = preds[:, 0, :]\n",
    "        return N_token\n",
    "\n",
    "    def summarize_attributions(attributions):\n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        attributions = attributions / torch.norm(attributions)\n",
    "        return attributions\n",
    "\n",
    "    def construct_whole_bert_embeddings(input_ids, ref_input_ids):\n",
    "    \n",
    "        input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "        #, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "        ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids)\n",
    "        #, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(model, 'roberta.embeddings')\n",
    "    try:\n",
    "        ablator = IntegratedGradients(custom_forward)\n",
    "\n",
    "        input_tensor = torch.tensor([input_seq] * len(top_N_tokens))\n",
    "#        print(input_tensor.shape)\n",
    "        ref_tensor = torch.tensor([ref_token_id]).expand((1,len(input_seq)))\n",
    "#        print(ref_tensor.shape)\n",
    "        interpretable_input_tensor = interpretable_embedding.indices_to_embeddings(input_tensor)\n",
    "#        print(interpretable_input_tensor.shape)\n",
    "        ref_tensor = interpretable_embedding.indices_to_embeddings(ref_tensor)\n",
    "#        print(ref_tensor.shape)\n",
    "\n",
    "        attention_mask = torch.ones_like(input_tensor)\n",
    "        attributions = ablator.attribute(\n",
    "                inputs=interpretable_input_tensor,\n",
    "                baselines=ref_tensor,\n",
    "                additional_forward_args=(attention_mask,),\n",
    "                target=top_N_tokens,\n",
    "        )\n",
    "        attributions = summarize_attributions(attributions)\n",
    "    finally:\n",
    "        remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
    "\n",
    "    return attributions.T\n",
    "attr_models = {\n",
    "    'Occlusion': occlusion_run_attribution_model,\n",
    "    'LayerIntegratedGradients': lig_run_attribution_model,\n",
    "    'IntegratedGradients': ig_run_attribution_model,\n",
    "    'ShapleyValueSampling': svs_run_attribution_model,\n",
    "#    'DeepLiftShap': dls_run_attribution_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_dataframe(model, attributions, input_seq, top_N_tokens):\n",
    "\n",
    "    ix = pd.CategoricalIndex(\n",
    "                tokenizer.convert_ids_to_tokens(input_seq),\n",
    "                categories=reversed(tokenizer.convert_ids_to_tokens(input_seq)),\n",
    "                ordered=True\n",
    "            )\n",
    "#    print(ix)\n",
    "    attr_df = (\n",
    "        pd.DataFrame(\n",
    "                attributions.detach().numpy(), \n",
    "                columns=tokenizer.convert_ids_to_tokens(top_N_tokens),\n",
    "                index=ix,\n",
    "            )\n",
    "        .reset_index()\n",
    "    )\n",
    "    attr_df = attr_df.melt(id_vars=[\"index\"])\n",
    "    #attr_df = attr_df[~(attr_df['index'] == \"<mask>\")]\n",
    "    attr_df['variable'] = pd.Categorical(\n",
    "            attr_df['variable'], \n",
    "            categories = tokenizer.convert_ids_to_tokens(top_N_tokens), \n",
    "            ordered=True\n",
    "        )\n",
    "    attr_df['display_value'] = attr_df['value'].apply(lambda f: f\"{f:.2f}\")\n",
    "    attr_df['model'] = model\n",
    "    return attr_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_plot(attr_df, mask_index, N):\n",
    "#    ncol = np.ceil(np.sqrt(N))\n",
    "    return (ggplot(attr_df, aes(x=\"index\", y=\"value\")) + \n",
    "        geom_col(aes(fill=\"index\", colour=\"index\"))  +\n",
    "        geom_text(aes(y=\"value/2\", label=\"display_value\"), size=10) +\n",
    "#        geom_label(aes(x = len(input_seq) - mask_index, y= 0, label=\"variable\"), size=13, boxstyle=\"darrow\") +\n",
    "        scale_x_discrete(drop=False) +\n",
    "#        facet_grid(\"model~variable\") +\n",
    "        facet_wrap(\"~model+variable\", scales=\"free_x\", ncol=N) +\n",
    "        coord_flip() +\n",
    "        labs(\n",
    "                x=\"Token in sentence\",\n",
    "                y=\"Captum contribution scores\",\n",
    "                title=\"Exploring the contribution of each token to the prediction.\"\n",
    "            ) +\n",
    "        theme(legend_position=\"none\", subplots_adjust={'hspace': 0.25})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_in_sentence(df, mask_index):\n",
    "    output = \"\"\n",
    "    for model, mg in df.groupby('model'):\n",
    "        output += \"<div style='font-size: 1.5em; padding: 1em; background-color: #CCC'>\"\n",
    "        output += f\"<h3>{model}</h3>\"\n",
    "        for v, g in mg.groupby('variable'):\n",
    "            output += f\"<div style='margin: 0.5em; padding: 0.5em; background-color: white; border: 3px solid #CCC; border-radius: 0.3em'>\"\n",
    "            for i, row in g.iterrows():\n",
    "                if i % len(g) == mask_index:\n",
    "                    output += f\"<span style='padding: 0.1em; text-decoration:underline;  background-color: rgba({'0,255,0' if row['value'] > 0 else '255,0,0'},{row['rel_value']});'>{row['variable']}</span>\"\n",
    "                else:\n",
    "                    word = row['index'].strip()\n",
    "                    word = re.sub(r'<', '&lt;', re.sub(r'>', '&gt;', word))\n",
    "                    output += f\"<span style='padding: 0.1em; background-color: rgba({'0,255,0' if row['value'] > 0 else '255,0,0'},{row['rel_value']});'>{word}</span>\"\n",
    "            output += \"</div>\"\n",
    "        output += \"</div>\"\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_show(sentence, mask_tokens, reference=tokenizer.unk_token_id):\n",
    "    if isinstance(mask_tokens, int):\n",
    "        N = mask_tokens\n",
    "        input_seq, mask_tokens, mask_index = prepare_data_for_sentence(sentence, N)\n",
    "    else:\n",
    "        N = len(mask_tokens)\n",
    "        if isinstance(mask_tokens[0], str):\n",
    "            mask_tokens = tokenizer.convert_tokens_to_ids(mask_tokens)\n",
    "        input_seq, _, mask_index = prepare_data_for_sentence(sentence, N)\n",
    "\n",
    "    result_df = None\n",
    "    for model, model_func in attr_models.items():\n",
    "        attributions = model_func(input_seq, reference, mask_tokens)\n",
    "        df = build_dataframe(model, attributions, input_seq, mask_tokens)\n",
    "        df['rel_value'] = (df['value'].abs() / df['value'].abs().max()).round(2)\n",
    "        if result_df is None:\n",
    "            result_df = df\n",
    "        else:\n",
    "            result_df = pd.concat([result_df, df])\n",
    "\n",
    "    plot = create_plot(result_df, mask_index, N)\n",
    "    display(plot)\n",
    "    show_in_sentence(result_df, mask_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"horse\", \"Ġhorse\", \"Ġbicycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"The\", \"author\", \"talked\", \"to\", \"Sarah\", \"about\", \"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"the\", \"Ġthe\", \"ĠThe\", \"Ġauthor\", \"Ġtalked\", \"Ġto\", \"ĠSarah\", \"Ġabout\", \"Ġbook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([19471, 5253, 14678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_and_show(\"John rode his <mask>.\", [\"horse\", \"Ġhorse\", \"Ġbicycle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_and_show(\"The author talked to Sara about <mask> book.\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_and_show(\"The author talked to Sara about <mask> experience.\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(\"The author talked to Sarah about her book.\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "for id, token in zip(ids, tokens):\n",
    "    print(id, token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"The author talked to Sarah about her book\".split()\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "actual_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "for id, token, actual in zip(ids, tokens, actual_tokens):\n",
    "    print(id, f\"'{token}'\", f\"'{actual}'\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_and_show(\"The author talked to Sarah about <mask> book.\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_and_show(\"Sarah talked to the author about <mask> book.\", [\"his\", \"Ġhis\", \"Ġher\", \"her\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
