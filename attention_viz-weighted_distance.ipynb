{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title foo\n",
    "!pip install transformers==4.1.1 plotnine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting stuff up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "plotnine.options.figure_size = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to force CPU if you have a GPU but not enough memory to do what you want. it will be slow of course\n",
    "\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = \"distilbert-base-cased\"\n",
    "transformer = \"bert-base-cased\"\n",
    "#transformer = \"gpt2\"\n",
    "#transformer = \"gpt2-medium\"\n",
    "#transformer = \"gpt2-large\"\n",
    "#transformer = \"twmkn9/bert-base-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer)\n",
    "model = AutoModel.from_pretrained(transformer, output_attentions=True, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This is a really long sentence that doesn't make much sense, but let's see what happens at the end\",\n",
    "    \"There are five subspecies of the pigeon guillemot; all subspecies, when in breeding plumage, are dark brown with a black iridescent sheen and a distinctive wing patch broken by a brown-black wedge.\",\n",
    "    \"Buchanan, working through federal patronage appointees in Illinois, ran candidates for the legislature in competition with both the Republicans and the Douglas Democrats.\",\n",
    "#    \"Less is more.\",\n",
    "]\n",
    "\n",
    "# gpt2 doesn't do padding, so invent a padding token\n",
    "# this one was suggested by the error you get when trying\n",
    "# to do masking below, but it shouldn't matter as the actual\n",
    "# tokens get ignored by the attention mask anyway\n",
    "if transformer in ['gpt2', 'gpt2-medium', 'gpt2-large']:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_dict = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "for k, v in input_dict.items():\n",
    "    input_dict[k] = v.to(device)\n",
    "print(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "att = np.array([a.cpu().detach().numpy() for a in output['attentions']])\n",
    "print(att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the data ready to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the attention mask to flag the padding tokens\n",
    "att_mask = input_dict['attention_mask'].cpu().detach()\n",
    "print(att_mask.shape)\n",
    "print(att_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = att.swapaxes(2,1)\n",
    "print(att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dimension indices of the array explicit as\n",
    "# a pandas dataframe MultiIndex\n",
    "spec = att.shape\n",
    "dims = {}\n",
    "for dim, size in reversed(list(enumerate(spec))):\n",
    "    if dim == len(spec) - 1:\n",
    "        dims[dim] = np.arange(size) + 1\n",
    "    else:\n",
    "        for d in range(dim + 1, len(spec)):\n",
    "            dims[d] = np.tile(dims[d], size)\n",
    "        dims[dim] = np.repeat(np.arange(size) + 1, np.prod(spec[dim+1:]))\n",
    "\n",
    "ix = pd.MultiIndex.from_arrays(list(dims.values()), names=reversed(['layer', 'head', 'sentence', 'from_token', 'to_token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        att.flatten(), # turn the array into one long list of numbers\n",
    "        columns=[\"attention_fraction\"], \n",
    "        index=ix, # indexed by its dimensions\n",
    "    ).reset_index() # and then turn the dimensions into columns\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter out the masked tokens\n",
    "for sentence, toklist in enumerate(att_mask.tolist()):\n",
    "    # the next two lines filter out the first and last unmasked token which are [CLS] and [SEP] (for bert)\n",
    "    # comment them out to see the results with them included\n",
    "    final = max(np.nonzero(toklist)[0])\n",
    "    modified = [0] + toklist[1:final] + [0] + toklist[final+1:]\n",
    "    for token in [i for i, v in enumerate(modified) if v == 0]:\n",
    "        df = df.query(f\"~(sentence == {sentence + 1} & (to_token == {token + 1} | from_token == {token + 1}))\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the weighted distances and their median per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = (df['from_token'] - df['to_token']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the data for the second token of the first sentence for the first layer and the first head\n",
    "# which is really the first one in the data as I filter out [CLS] above\n",
    "with pd.option_context(\"max_rows\", None):\n",
    "    display(df.query(\"layer == 1 & head == 1 & sentence == 1 & from_token == 2\").sort_values(\"attention_fraction\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weighted'] = df['distance'] * df['attention_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby(['layer', 'head'])\n",
    "median_dist = (g['weighted'].median()).reset_index().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(median_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot\n",
    "\n",
    "Cheat a bit by limiting the plots to show only the interval between 0 and 2 for the weighted distance. \n",
    "\n",
    "This unsquishes the violins to reveal a pattern a bit similar to the plot of `k` in the Hopfields networks\n",
    "paper, but it does hide values (especially for the dot plot), which may give a false impression\n",
    "\n",
    "you can change the interval by adjusting the limits variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it!\n",
    "limits = (0, 2)\n",
    "plotnine.options.figure_size = (20, 20)\n",
    "(ggplot(df, aes(1, \"weighted\"))  + \n",
    "     geom_jitter(height=0, size=0.1, alpha=0.1, color=\"magenta\") +\n",
    "     geom_violin(fill=\"lightblue\") + \n",
    "     geom_label(data=median_dist, mapping=aes(x=1.2, y=limits[1] * .75, label=\"weighted\")) +\n",
    "     scale_y_continuous(breaks=np.linspace(*limits, num=3)) +\n",
    "     facet_grid(\"layer ~ head\", labeller=\"label_both\") + \n",
    "     coord_flip(ylim=limits) +\n",
    "     labs(\n",
    "             x = \"\",\n",
    "             y = \"weighted distance\",\n",
    "             title = \"Distribution and median of distances between attending and attended tokens\"\n",
    "         ) +\n",
    "     theme(\n",
    "             axis_text_y = element_blank(),\n",
    "             axis_ticks_major_y = element_blank()\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from me trying to figure out why the plot looked weird when I used mean instead of median\n",
    "pd.concat([\n",
    "    df[(df['head'] == 1) & (df['layer'] == 5)]['weighted'].describe(),\n",
    "    df[(df['head'] == 1) & (df['layer'] == 6)]['weighted'].describe(),\n",
    "    df[(df['head'] == 1) & (df['layer'] == 7)]['weighted'].describe(),\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset of heads\n",
    "\n",
    "plot only a few heads, so each facet can be bigger and it's not as neccesary to limit what is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"head == 1 & layer >= 5 & layer <= 7\"\n",
    "plotnine.options.figure_size = (20, 6)\n",
    "(ggplot(df.query(subset), aes(1, \"weighted\"))  + \n",
    "     geom_jitter(height=0, alpha=0.1, color=\"magenta\") +\n",
    "     geom_violin(fill=\"lightblue\") + \n",
    "     geom_label(data=median_dist.query(subset), mapping=aes(x=1.2, y=df.query(subset)['weighted'].max() * 0.75, label=\"weighted\")) +\n",
    "#     facet_grid(\"layer ~ head\", labeller=\"label_both\") + \n",
    "     facet_wrap(\"~ layer + head\", labeller=\"label_both\") + \n",
    "     coord_flip(\n",
    "#             ylim=(0,10)\n",
    "         ) +\n",
    "     labs(\n",
    "             y = \"\",\n",
    "             title = \"Distribution and median of distances between attending and attended tokens\"\n",
    "         ) +\n",
    "     theme(\n",
    "             axis_text_y = element_blank(),\n",
    "             axis_ticks_major_x = element_blank()\n",
    "         )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
